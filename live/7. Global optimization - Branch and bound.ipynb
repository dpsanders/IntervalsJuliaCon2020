{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Global optimization: Branch and bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the branch-and-prune algorithm to solve another fundamental numerical problem: **global optimization**: for a function $f: \\mathbb{R}^n \\to \\mathbb{R}$,\n",
    "\n",
    "> minimize f(x) over $x \\in X \\subseteq \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to have both the global minimum *value* that the function takes, as well as the **global minimiz*ers***, i.e. the locations where it takes that value.\n",
    "\n",
    "This is a very difficult problem for general nonlinear, non-convex functions $f$. Interval methods provide one of the only methods for guaranteeing that you have found the true global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Spatial) branch and bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic mechanism of the **branch-and-bound* algorithm is very similar to that of branch-and-prune: we repeatedly bisect and analyse each box.\n",
    "\n",
    "Now, however, we do not have such a simple condition to check as \"does $f(X)$ contain $0$?\". We need to think a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we evaluate $f$ at a *point* $x$. Then clearly the global minimum value $m^*$ must satisfy $m^* \\le f(x)$.\n",
    "In other words, $f(x)$ is an *upper bound* for the global minimum $m^*$. [We need to evaluate $f(x)$ using interval arithmetic to bound rounding errors.]  We will keep a running upper bound $m$ which we know satisfies $m^* < m$. For example, if we find an $x$ for which $f(x) < m$ then we set $m := f(x)$. [Strictly, $m := \\sup(f(x..x))$.]\n",
    "    \n",
    "Now think about a box $X$ in the branching process. If we evaluate $f(X)$ and find that the result lies strictly *above* $m$ then $\\text{range}(f, X)$ also lies above $m$, so that $f(x) > m \\ge m^*$ for all $x \\in X$. Thus $X$ *cannot* contain a global minimizer, and can be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement this.\n",
    "\n",
    "\n",
    "2. Apply it to the function $f(x) = (x^2 - 2)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a priority queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For interval optimization, much more than for root finding, the order in which boxes are dealt with turns out to be very important: the faster we can find candidate points $x$ with lower values of $f(x)$, the more boxes we can exclude, or \"fathom\", quicker. \n",
    "\n",
    "One popular choice is to order the boxes by the infimum (lower bound) of $f(X)$, the heuristic idea being that a box with a lower minimum value of $f(X)$ is more likely to contain a deeper global minimum.\n",
    "Many papers have explored alternative heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suitable data structure to order boxes in this way is `PriorityQueue` from `DataStructures.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IntervalOptimisation.jl package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note the `s` in the package name, corresponding to British spelling. We may want to change this in the future.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `IntervalOptimisation.jl` package contains an implementation of the above algorithm, using a priority queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using IntervalOptimisation, IntervalArithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1.40858e-07], Interval{Float64}[[-1.41471, -1.41407], [1.41377, 1.41439]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(x->(x^2 - 2)^2, -10..10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the function `minimize` to find the global minimum of $f(x) = (x^2 - 2)^2$.\n",
    "\n",
    "\n",
    "2. What does it return?\n",
    "\n",
    "\n",
    "3. Minimize the Himmelblau function,\n",
    "\n",
    "$$f(x, y) = (x^2+y-11)^2 + (x+y^2-7)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cluster effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above algorithm is that it usually ends up generating a *cluster* of boxes around the global minimum (or minima). This is due to the dependency effect which means that interval evaluations of functions give over-estimates; hence the algorithm is unable to exclude boxes. Increasing the tolerance often results in *more* boxes.\n",
    "\n",
    "A solution to this is to use a \"more accurate\" inclusion function. (An inclusion function is a function that returns an enclosure of $\\text{range}(f; X)$.)\n",
    "\n",
    "One option is a **mean-value form**. Effectively this is an enclosure of the first-order Taylor expansion of $f$ around the midpoint of $X$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_{\\text{mvf}}(X) = f(m(X)) + f'(X) * (X - m(X))$$\n",
    "\n",
    "Here, $m(X)$ is again the midpoint of $X$. (Other points may also be used.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that when the size of $X$ goes to $0$, the overestimate of the true range converges faster (quadratically) using the mean-value form compared to the standard interval extension."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0-beta1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
